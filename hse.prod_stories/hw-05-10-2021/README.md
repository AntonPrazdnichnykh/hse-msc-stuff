# Домашнее задание к лекции курса "Истории из продакшена" от 05.10.2021

## Функция вознаграждения
Эксперименты проводились с 2-мя видами награды:
<ol>
    <li>
        Basic (уже была реализована):
        <img src="http://www.sciweavers.org/tex2img.php?eq=%20x%20%3D%5Cfrac%7Bnew%5C%3Bexplored%5C%3Bcells%7D%7Bvisible%5C%3Bcells%7D&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0" align="center" border="0" alt=" x =\frac{new\;explored\;cells}{visible\;cells}" width="201" height="43" />
    </li>
    <li> 
        Complex (взята из лекции):
        <img src="https://bit.ly/3b2aGsT" align="center" border="0" alt=" x =\begin{cases}0.1  + 20\cdot\frac{explored\;cells}{visible\;cells}, & if\;new\;cell\;visited\\-1, & if\;bumped\;into\;tile\\-0.5, & else\end{cases} " width="407" height="79" />
    </li>
</ol>

Я решил взять такую награду, т.к. проблема базовой заключается в том, что на поздних шагах траектории, когда 
неисследованных клеток осталось мало, такая награда становится маленкой и разреженной, что делает нахождение этих 
оставшихся клеток крайне сложным для алгоритма. Эта же проблема возникала и при решении задачи описанной в лекции, и
было предложено ее решение с помощью награды из п.2 выше. Кажется, что задачи из лекции и из домашки довольно похожи,
поэтому я и решил использовать ее.

## Код и логирование
Для запуска когда необходимо выполнить следующие команды:

`git clone https://github.com/AntonPrazdnichnykh/hse.prod_stories.git
`

`cd hse.prod_stories/hw-05-10-2021/rl_explore`

`pip install -r requirements.txt`

`python train.py`

Ход обучения логируется в соответствующий [wandb проект](https://wandb.ai/strange_attractor/prod-stories-05-10?workspace=user-strange_attractor).

На главной странице приведены графики изменеия энтропии, лосса актора, лосса критика и результирующего лосса, а также 
средней награды. Также на панели "trajectories" там логируются анимации траекторий.
Конфиг конкретного эксперимента и чекпоинты модели можно найти, перейдя в конкретный "run", на вкалдке "files". Например,
для эксперимента "basic-rew-longer-ep" соответствующая ссылка выглядит так: https://wandb.ai/strange_attractor/prod-stories-05-10/runs/624c5hsx/files

## Описание экспериментов
Было проведено 2 основных эксперимента, соответсвтующих 2м функциям вознаграждения: базовой (basic-rew-longer-ep) и 
измененной (complex-reward). Арихитектура актора и критика была стандартной:
> Conv2d(channels=16, kernel_size=3, stride=2)\
> Сonv2d(channels=32, kernel_size=3, stride=2)\
> Conv2d(channels=32, kernel_size=3, stride=1)\
> Dense(32)\
> Dense(out_dim)

Где out_dim = action_dim для актора и 1 для критика.

Алгоритм для обучения агента -- [PPO](https://openai.com/blog/openai-baselines-ppo/)
 
Из экспериментов видно, что средняя награда а случае complex награды продолжает в среднем расти на протяжении всего
обучения, а для basic награды, награда почти сразу выходит на плато и дальше не растет. Это может служить слабым аргументом
в пользу выбора complex награды. Слабым потому, что по нельзя сказать чтобы визуализации траеторий для complex награды были 
сильно лучше, чем для basic. Возможно, это связано с тем, что, как было отмечено ранее, средняя награда для complex награды 
продолжает расти и на последних шагах обучения, и в этом смысле можно сказать, что процесс обучения еще не сошелся. 
Количество шагов обучения я взял примерно с потолка в количетсво 300000. В дальнейших экспериментах можно попробовать
увеличить этот гиперпараметр и посмотреть, что получится. К слову, то же самое справедливо и в отношении выбора других гиперпараметров.
Так, например, видно, что лосс актора и результирующий лосс не особо затухают в ходе обучения, что также может быть связано 
с неудачным выбором гиперпараметров оптимизации, в частности, learning rate актора и критика. Неплохо было бы провести
несколько экспериментов с целью определить какие гиперпарметры обучения разумны для данной задачи. 

Если после оптимизации гиперпараметров алгоритм  все еще не заведется, то, скорее всего, надо будет искать решение в
направлении более умного выбора награды и, возможно, модификации наблюдений, получаемых из среды (я  использовал те,
что поступают по умолчанию).

